# Trex

# Introduzione

La connessione permanente ad internet che ormai sperimentiamo è una novità. La rete è un formidabile
strumento di raccolta dati; scollegarsi non è un'opzione praticabile, a meno di non rinunciare a *quasi* ogni servizio
possibile, dalla relazioni con le proprie cerchie, al noleggio di un'automobile o la gestione di un conto in
banca. Inoltre, la recente pandemia globale ha imposto l'introduzione del distanziamento sociale, aumentando di frequenza
ed importanza le pratiche di connessione remota come la teledidattica, il telelavoro e gli acquisti online.

Ad ogni connessione gli individui lasciano dietro di sé un grande numero di dati e metadati, che vengono utilizzati come
merce dalle ditte private che li raccolgono tramite servizi "gratuiti", come ad esempio i social network. Questi dati
sempre aggiornati, aggregati in grandi quantità, vengono chiamati "Big Data" e sono commerciabili in quanto di grande
utilità per disegnare il carattere e le abitudini degli individui; vengono utilizzati anche per individuare, stimolare e
indirizzare il consenso sia in area di marketing che di governance politica.

La capacità di raccogliere grandi quantità di dati è privilegio delle entità che possiedono i mezzi di produzione adatti
a farlo e queste entità non corrispondono alle istituzioni locali o statali, ma sono ditte private come ad esempio
Google, Amazon e Facebook. Esse posseggono, trattano e rivendono i dati in un'ottica di profitto privato che non necessariamente corrisponde a un bene comune. Ci troviamo di fronte ad un'asimmetria informativa e dunque di potere. Le leggi
sulla Privacy rincorrono e non precedono l'innovazione (scientifica?) e sono inadeguate a rapportarsi con il potere delle
entità private sovente extra nazionali che posseggono i dati dei cittadini.

Il modo in cui i Big Data vengono raccolti e trattati, le modalità di analisi dei dati, chiama in causa la parola
Algoritmo. Gli algoritmi sono calcoli finiti che hanno lo scopo di trarre conclusioni, come una ricetta. Sappiamo che
una ricetta non è neutrale, come non lo è la Statistica e non lo è l'algoritmo. Il punto d'osservazione, le modalità di
acquisizione, la capacità e la volontà di cercare o dimostrare qualcosa influiranno sul risultato finale, che verrà poi
fornito formattato per ulteriore interpretazione. Il modo in cui viene presentata un'informazione contiene già un
giudizio, un punto di vista (più che punto di vista io direi giudizio di valore o piuttosto visione del mondo) e un obiettivo.

Gli algoritmi non sono pubblici e il loro funzionamento è
segreto aziendale. Non sappiamo con quali criteri ci vengono raccomandati video su Youtube.
Possiamo intravedere lo scopo pubblicitario, ma non quello politico. Non sappiamo ancora come funziona, ma ci siamo
accorti come queste informazioni siano utilizzate per determinare il consenso e per regolare le comunità al suo interno.

Il caso Cambridge Analytica: nel 2018, viene
rivelato che la società Cambridge Analytica ha usato a scopo politico i dati degli utenti, fornendo loro informazioni su
misura, azione che nel loro gergo tecnico viene detta segmentazione, allo scopo di influenzare le scelte elettorali. La
società riceveva i dati da Facebook, ufficialmente senza pagarli ma come ricercatori (?). Negli anni è intervenuta per
influenzare le elezioni in Argentina, Nigeria, Italia e Stati Uniti. Per stessa ammissione raccolta in video del
responsabile Alexander Nix, le attività di gestione del consenso non si limitavano all'uso dei dati, ma comprendevano
attività più palesemente illecite, come l'utilizzo di sex workers per ricattare i candidati politici scomodi. Lo
scandalo travolge Cambridge Analytica, chiudendola, ma anche Facebook. Quando il suo fondatore Mark Zuckerberg sarà
interrogato al Senato USA, alla domanda su quale sia la natura dei suoi affari risponderà: "Senatore, noi facciamo
pubblicità - Senator, we run ads". Fino ad allora Facebook non si riferiva a sé come a un'agenzia pubblicitaria, ma come
a un servizio gratuito per mettere in collegamento le persone nel mondo. Il suo potere è altro ancora: Facebook ha la
possibilità di influenzare la politica. (aggiungerei anche: la nostra visione del mondo e i nostri comportamenti PER I LORO INTERESSI).

Tracking Exposed nasce nel 2016, per analizzare dall'esterno (??) il funzionamento di un algoritmo.

Per sviluppare un'etica dell'informazione dobbiamo essere in grado di interpretare il funzionamento degli algoritmi. Non avendo accesso al codice sorgente degli algoritmi, possiamo applicare il
metodo scientifico creando una situazione mirata (cioè?), raccogliere i dati che ne scaturiscono e confrontarli. Si, stiamo
tirando palline da ping pong verso un muro invisibile e cerchiamo di capire la natura del muro studiando le palline che
ci ritornano.

Una delle funzioni degli algoritmi è di tracciare i comportamenti personali e fornire risposte diverse a persone
diverse, ad esempio in quale diverso ordine vengono proposti i post sulla timeline di Facebook. Per questo abbiamo
creato un'estensione per browsers (ovvero add-on) in modo che su base volontaria le persone potessero fornirci i dati che Facebook crea
durante l'esperienza di utilizzo, e rilevare le eventuali diversità di trattamento tramite il confronto. (così pare siate in grado di raccogliere tutti i dati che facebook crea durante la navigazione. non credo proprio. sbaglio?).

Lo scopo esplicito di Tracking Exposed è di rivelare la natura ed il funzionamento delle tecnologie di tracciamento,
dato che le conseguenze sul mondo reale sono ormai innegabili/profonde/sistemiche. È un compito difficilmente risolvibile con un approccio legislativo e
certamente non si può chiedere alla stessa entità che trae profitto dall'uso dell'algoritmo come segreto industriale, di
spiegarci il suo funzionamento, ancor di più illusorio sarebbe credere che possa, senza svelarne la natura, mitigare le
conseguenze o la nocività, in quanto sarebbe naturalmente antitetico all'interesse privato dell'azienda.

Ogni (un pò troppo generalizzante ed estremo? io direi quasi ogni..) tentativo di interazione di tipo legislativo si è rivelato fino ad oggi insufficiente, ma di fronte
all'obsolescenza delle leggi - il legislatore segue l'innovazione tecnologica, non può precederla - l'approccio non può
essere puramente di tipo legislativo, inoltre l'aumentare dell'automazione nella burocrazia di ambito anche giuridico,
l'uso delle intelligenze artificiali (perché plurale?) in ambito normativo e la tendenza a costruire regole basandosi sui dati (data
driven policy) rende urgente avere la possibilità di conoscere e poter decostruire gli elementi che partecipano alla
formazione dei dati. Occorre incamminarsi verso un tipo di etica dell'informazione che consideri l'individuo come agente
libero e responsabile, portatore di un pacchetto di informazioni di cui va salvaguardata l'integrità, non solo la
proprietà, e che sia questo approccio etico a guidarci nelle scelte future. Occorre che le istituzioni di monitoraggio
etico-politico (watchguard) (cioè? a me pare uno dei problemi è che manchino tali istituzioni; per esempio, trex dovrebbe essere un'istituzione pubblica a mio parere) siano in grado di effettuare le analisi in proprio e in maniera indipendente, sia possedendo
gli strumenti tecnici che il metodo di analisi. Perché siano in grado di collaborare tra loro e con la comunità
scientifica.

Sappiamo che la tecnologia non è neutrale (ripetizione. io mi sottolinerei di più l'aspetto politico e più che tecnologia di design), che l'intelligenza artificiale fallisce nel comprendere contesto e intento (?) e
che l'algoritmo è oggi strumento di governance politica (non mi è chiaro tale concetto, in che senso governance?). Un discorso di decostruzione del potere ha bisogno di farsi
comprendere, per questo si avrà un approccio interdisciplinare (ma più che farsi comprendere - aumentando la complessità concettuale - l'interdisciplinarietà serve per comprendere. cioè, in generale, l'interdisciplinarietà aiutando a comprendere serve a far comprendere. non credo che problematizzando il lettore comune comprenda più facilmente, anzi). Per comprendere e farci comprendere chiederemo aiuto
alla sociologia, alla semiotica, alla psicologia (ma più che alle discipline io direi offriremo una sintesi onnicomprensiva chiedendo aiuto a sociologi, psicologi - e dato che che si cita tanto l'etica - a filosofi - altrimenti non ha senso secondo me) ma prima ancora crediamo che servano evidenze tecnologiche ricavate
empiricamente (così pare che si dia un valore superiore all'analisi empirica su quella teorica, che a mio parere è sbagliato: si tratta di una sinergia inscindibile): nasce tracking exposed. (prima si usa un futuro poi il presente: meglio 'per questo è nato il progetto ...'?)

Gli algoritmi organizzano il tempo e il consenso. Non esiste una soluzione tecnica (tecno-soluzionismo) perché Il
problema non è solo tecnico, ma sociale, giuridico, politico e tecnico. La rete non è uno spazio staccato dalla realtà,
le conseguenze della repressione algoritmica variano a seconda della situazione (geo)politica dove si esprimono, ma
ricadono sempre (sempre?) e con grande violenza sul corpo (in che senso?). Ad esempio, un post "sbagliato" su Facebook in Italia può causare un
licenziamento, ma in un Paese in guerra può costare la vita.

Crediamo che ai Big Data si debba rispondere con dei 'Big Rights'. È una questione politica e non solo economica. Per avere
dei diritti bisogna esercitarli e dunque conoscerli. I dati possono essere utilizzati, in un contesto democratico,
nell'interesse collettivo (common good), in attesa che questo sia possibile, vogliamo contribuire a tenere aggiornata la
pubblica opinione e la comunità scientifica sulle dinamiche dello sfruttamento dati e stimolare un dibattito a questo
proposito. Lo scopo di Tracking Exposed è di evidenziare e rendere esercitabili i diritti legati allo sfruttamento dei
dati, fare informazione e sviluppare critica, allo stato attuale non possiamo ambire a promuovere soluzioni, ma a far
conoscere il funzionamento degli algoritmi proprietari, il quale è un requisito necessario per ogni forma di democrazia
moderna, perché Internet è, dopotutto, anche un mezzo di oppressione. La causa della libertà nel secolo 21 è inestricabilmente
connessa alla resistenza alla sorveglianza elettronica (e, aggiungerei, alla trasparenza delle analisi dei dati raccolti).

# Posizioni citabili: spunti, slogan e le spalle dei giganti.

"Non si può mangiare un confetto pretendendo di sentire – solo perché si ha una vasta cultura e un forte controllo delle
proprie sensazioni – sapore di sale. La chimica non sbaglia mai. Siccome esiste anche una chimica delle emozioni, e uno
dei composti che per antica tradizione suscitano emozioni è un intreccio ben congegnato, se un intreccio è ben
congegnato suscita le emozioni che si era prefisso quale effetto. Potremo poi, après coup, criticarci per averle
provate, o criticarle come emozioni repellenti, o criticare le intenzioni con cui è stata congegnata la macchina che le
ha provocate. Ma questa è un'altra storia. Un intreccio ben temperato produce gioia, terrore, pietà, riso o pianto."  
Umberto Eco, Il superuomo di massa, 1976

"Quando la signora Thatcher è stata eletta per la seconda volta, aveva ingaggiato la Saatchi & Saatchi, una grossa
compagnia pubblicitaria, per la sua campagna. E i pubblicitari hanno utilizzato tutti i possibili trucchi, dai giri di
frase calcolati per suscitare facili emozioni, ai colori dei suoi vestiti o delle tende davanti alle quali si metteva,
fino al calcolo preciso di quando comparire e scomparire dai media. E intanto la sua nobile opposizione socialista
disprezzava quei trucchi e i media. Abbiamo potuto osservare bene l'attenta regia della campagna della signora Thatcher,
seguendo un geniale programma televisivo. Quando dico "noi" intendo quella minoranza del paese che l'ha seguito, mentre
ritengo che guardarlo avrebbe dovuto essere obbligatorio. Siamo arrivati a un punto in cui un leader politico non solo
usa abilmente i vecchi trucchi teatrali per fomentare la folla, come faceva Giulio cesare nel dramma di Shakespeare, ma
ingaggia gli esperti che rendono tutti quei trucchi più efficaci. L'antidoto però consiste nel fatto che in una società
aperta possiamo anche esaminare quei trucchi mentre vengono usati con noi. Naturalmente sempre che scegliamo di
esaminarli, sempre che non cambiamo canale per vedere Dallas o qualsiasi altra cosa."  
Doris Lessing, le Prigioni che abbiamo dentro, 1987

"Mentre le merci sono separate da noi e possono essere portate a casa per essere analizzate prima di essere consumate,
l'informazione entra a far parte di noi non appena viene acquistata, e provoca in noi cambiamenti determinanti, se
considerati nella prospettiva dell'intellettualismo etico (314b), per la quale l'ignoranza e la disinformazione sono la
differenza decisiva fra il male e il bene".  
Il [Protagora][] di Platone di Maria Chiara Pievatolo
[Protagora]: https://btfp.sp.unipi.it/dida/protagora/ar01s02.xhtml#mercimathema

"Libertà non sta nello scegliere tra bianco e nero, ma nel sottrarsi a questa scelta prescritta". Theodor Adorno

"Conoscere gli individui meglio di quanto conoscano sé stessi è un formidabile mezzo di controllo".
Edward Herman e Noam Chomsky, Manufacturing consent, 1988

"L'individuo in rete è rappresentato dall'insieme dei sui dati".
Stefano Rodotà

"Internet non è più uno spazio libero e indipendente, ma è commercialmente controllato e personalizzato, Google e
Facebook ciberanno gli utenti di ciò che vogliono che veda, il computer è diventato uno specchio che riflette i tuoi
interessi e rinforza i tuoi pregiudizi".  Eli Pariser, Filter bubble, 2011.

# gli Algoritmi di personalizzazione

Gli algoritmi di personalizzazione sono nati perché le compagnie volevano rendere l'esperienza online meno noiosa, piú
efficiente e soddisfacente (efficienza=ottimizzazione engagement e soffisfazione sono inversamente proporzionali a mio parere, quindi più che meno noiosa e soddisfacente, la volevano più eccitante e gratificante, che invece sono emozioni allineate con l'ottimizzazione). Gli algoritmi sono serviti ad evitare lo spam (eh, insomma, manco questo!!). Oggi regolano il discorso pubblico
all'interno dei social media e di conseguenza sulla governance, fino all'individuo. Disincentivano il pensiero
critico: se radio e la tv sono strumenti di manipolazione di massa, amalgamatore di immaginari ed unificatore
linguistico, almeno si può dire dell'internet della raccolta dati (?). Che sia per limitare l'information
overload (in italiano sovraccarico cognitivo, metterei entrambe, una tra parentesi, ci vuole una funzione didattica secondo me, empatizzando con chi non conosce affatto questi concetti, in inglese poi..), o che sia per tenere l'utente incollato allo schermo e vendere più pubblicità, l'algoritmo svolge una funzione
di gatekeeping non solo automatizzato, ma i cui presupposti ideologicizzati non sono accessibili. (che in fin dei conti son probabilmente diversi da invididuo a individuo, da gruppo sociale a gruppo sociale, e da nazione a nazione - tipo una personalizzazione della personalizzazione)

Data la loro complessità, gli algoritmi non sono comprensibili dal pubblico e inoltre sono segreti, le compagnie non
cedono i dati e tantomeno le formule. È irrealistico chiedere alle compagnie di autoregolarsi. (è poco credibile, non irrealistico). Per verificare che le
imprese non mettano in atto pratiche scorrette nei confronti del consumatore si rivela necessaria un'analisi terza ed
indipendente sugli algoritmi che possono condizionare indebitamente. Inoltre non si può discutere di vessatorietà di
clausole contrattuali senza considerare come effettivamente i dati del consumatore vengono sfruttati per discriminare,
non solo sulla base di nazionalità e luogo di residenza, ma anche sulla base di dati molto più sensibili quali
orientamento sessuale, reddito e disponibilità economica, stato civile e livello di istruzione (ma aggiungerei numerose altre possibili inferenze oltre quelle molto generali). Le dichiarazioni delle
imprese non sono sufficienti. È data la possibilità di documentare e analizzare prove circa l'aggressività e la
pervasività di pratiche commerciali dai tempi e modalità personalizzate algoritmicamente. Gli algoritmi devono essere
conosciuti attraverso un'analisi esterna e indipendente.

Le conseguenze della repressione algoritmica sono in parte ancora da vedere (in parte? in grandissima parte!), ma sappiamo già che: "Conoscere gli
individui meglio di quanto conoscano sé stessi è un formidabile mezzo di controllo." Manufacturing consent, Edward
Herman e Noam Chomsky, 1988. Sappiamo anche che "Internet non è più uno spazio libero e indipendente, ma è
commercialmente controllato e personalizzato, Google e Facebook ciberanno gli utenti di ciò che vogliono che veda, il
computer è diventato uno specchio che riflette i tuoi interessi e rinforza i tuoi pregiudizi." Filter bubble, Eli
Pariser, 2011. 

[d] link al paragrafo su Cambridge analytica -oppure- discorso sui mutui, le assicurazioni vita.

# Intermezzo: l'Algoritmo Captivo

[d] Qui mi domando se partire con una cosa che per quanto mi piaccia, potrebbe non c'entrare nel contesto di questo
documento, ma potrebbe essere un articolo a parte. Mi riferisco alla considerazione della parola "cattivo" nel suo
significato originale di "captivo", ossia prigioniero.  E costruire un simpatico pensierino sul fatto che se non viene
"liberato", come può essere vista l'azione di Trex nello svelarne il funzionamento, l'algoritmo, prigioniero nei vincoli
tecnologici e legali che tengono oscuro il suo funzionamento, fa danno sociale. Dunque occorre liberarlo.  (liberare
l'algoritmo, per liberarci). Mi sembra carina, ma magari è più che altro uno spunto narrativo per scrivere un articolo
su Trex, me la segno per non scordarla e anche per sapere la vostra.

# Trex: Tracking Exposed

Siamo un progetto no-profit che usa software libero per analizzare prove di personalizzazione algoritmica. Permettiamo
agli utenti dei social media di setacciare e raccogliere i dati che gli vengono imboccati e di analizzarli per
comparazione, offrendo loro interfacce rispettose della privacy. Allo scopo di rivelare quanto sia aggressivo e
manipolatorio il moderno panorama di Internet e le sue conseguenze.

Trex investiga gli algoritmi. Riportiamo il potere di analisi e controllo dei dati agli utenti che i dati producono e
dovrebbero dunque possedere. Forniamo alle persone gli strumenti per controllare il controllore. Crediamo che per
attenuare la repressione algoritmica, da parte di chi dispone del potere di raccoglierli e utilizzarli, sia necessario
che il funzionamento degli algoritmi sia trasparente al pubblico. L'azione di Trex è di incisione sulla realtà,
rivelando il funzionamento dei sistemi di profilazione personalizzati e producendo prove del loro funzionamento. Lo
scopo di Trex è abilitare le persone, le quali vengono chiamate: "utenti" nel loro rapporto con la tecnologia di
consumo, a mantenere la loro parte di potere che è rappresentata dai loro dati.

Per fare tecnologia, ossia per programmare strumenti di analisi aggiornati e per fare educazione, divulgare gli
strumenti e i metodi di lavoro sia per altri analisti, che in ambito di ricerca, abbiamo bisogno di una copertura
economica che ci permetta di non basarci solamente sul volontariato, soprattutto considerando la statura materiale delle
entità commerciali che confrontiamo. Crediamo che il momento storico sia paragonabile al medioevo (nella comune e non
del tutto esatta accezione di epoca oscura e superstiziosa) per quanto riguarda gli strumenti digitali, l'uso della rete
e il suo impatto sulla società è in corso e crediamo che il nostro lavoro abbia un'importanza considerevole, che
pensiamo aumenterà d'importanza col tempo.

"La libertà non può essere gratis". Seneca

# Metodo

Tracking Exposed utilizza il metodo scientifico. Fa uso di tecnologie aperte e verificabili, rende pubblici e così
utilizzabili dal pubblico e dalla comunità scientifica i risultati delle analisi e gli strumenti utilizzati, la
metodologia, il contesto e i dati raccolti nella loro interezza perché siano sottoposti a ulteriore verifica. 
Accompagniamo le analisi a una descrizione trasparente per permettere replica e confutazione. Costruiamo in proprio gli
strumenti di analisi quando questi non sono già esistenti e li rendiamo disponibili, scaricabili e utilizzabili
liberamente senza alcun bisogno di autorizzazione da parte nostra. Siamo indipendenti nella raccolta dei dati, nella
scelta di quale obiettivi perseguire, della posizione di osservazione adottata e dei finanziamenti ricevuti rispetto
alle dinamiche algoritmiche osservate (vedi: la Storia di Trex). Tendiamo a un approccio interdisciplinare per tenere
conto della complessità dell'osservazione e per mantenere la capacità di comunicare i risultati, ma aderiamo al metodo
sperimentale di raccolta prove e analisi comparativa.

Utilizziamo lo schema scientifico di riproducibilità sull'apprendimento automatico, versione 2,7 Aprile 2020
[Reproducibility checklist][]: Per ogni modello e algoritmo presentato, viene inclusa una descrizione chiara degli
assunti matematici e dei modelli considerati, analisi della complessità che comprende: tempo, spazio e quantità di dati
per ogni algoritmo. Ogni ipotesi teorica comprende la prova in completezza e la sua descrizione. Ogni banca dati
utilizzata è accompagnata dalla statistica di rilievo, come numero di esempi, andamento, validazione e diversità dei
test effettuati; quali dati sono stati esclusi e perché, assieme ai passi preliminari. Pubblichiamo la versione
scaricabile dei dati e dell'ambiente di simulazione, ogni dato raccolto è accompagnato da una descrizione completa del
processo di collezione, incluse le istruzioni e i metodi per la verifica. I codici condivisi comprendono le specifiche
delle dipendenze, l'evoluzione del codice, preparazione, modelli e documento LEGGIMI con i comandi da eseguire per
riprodurre i risultati.

[Reproducibility checklist]: https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf

Il nostro scopo è non solo di fornire un metodo utilizzabile in ambito di ricerca o di studio, ma di parallelamente
permettere all'utilizzatore finale di compiere le sue proprie ricerche ed analisi, in quanto pensiamo che la capacità di
analisi dei dati sia da riportare alle persone cui i dati appartengono. Le persone usando la rete dovrebbero avere
persino la possibilità di comporre il loro, proprio algoritmo. Abbiamo scoperto che le nostre analisi hanno l'utilità di
decostruire l'azione delle piattaforme e in alcuni casi di permettere l'individuazione di campagne di disinformazione.
Riconosciamo tre categorie di attori sociali che chiamiamo: utenti, produttori di news e piattaforme. Dividiamo tra
dieta informativa e diversità informativa all'interno delle analisi di quella che riconosciamo come filter bubble.
Infine, abbiamo sperimentato l'opposizione delle piattaforme alle analisi che le riguardano, non meramente sul piano
legale, ma soprattutto su quello più materiale dell'opacità, assenza delle informazioni - il rilascio dati è sempre
incompleto - e addirittura su quello tecnocratico del renderle esplicitamente inaccessibili. L'analisi delle API non è
sufficiente e inoltre le compagnie aggiornano e chiudono le API quando vogliono, godendo della evidente asimmetria di
potere. Anche per questo stiamo transitando da una metodologia di analisi quantitativa verso un'analisi qualitativa e di
inserimento di ulteriori elementi contestuali.

Tracking Exposed rappresenta un punto d'osservazione **indipendente** e si propone di rendere visibile il tracciamento, cioè
il modo in cui i social network studiano le persone, raccolgono e processano i loro dati secondo attività e interazioni
per arrivare a proporre contenuti mirati. La sorveglianza non è visibile per le persone, perché gli strumenti di analisi
(algoritmi) sono oscuri. Questo produce discriminazione algoritmica. Nell'intento di rendere le nostre **analisi
comprensibili e utilizzabili** nel dibattito contemporaneo, tentiamo di non produrre solo dati percentuali, ma analisi
che, partendo da dati ricavati con metodo scientifico esperienziale (sperimentale), producano elementi di critica e di
utilità sociale.

La base del nostro metodo consiste nel **cercare evidenze** nel diverso trattamento automatizzato che ricevono due utenti
simili. Per ricavare i dati di utenti simili mentre accedono allo stesso servizio, abbiamo adottato sia la tecnica di
creare utenti appositamente, sia quella di chiedere su base volontaria di fornirci i dati. La **comparazione** tra i dati
raccolti da un browser pulito, senza tracce di navigazione precedente e uno già utilizzato, dunque contenente tracce
personalizzate, offre per la nostra analisi i risultati più interessanti.

**Descriviamo** in modo accurato il metodo con il quale effettuiamo il test; sia per lasciare a chi legge gli strumenti
necessari per poterlo replicare, che per avere la possibilità di mettere in discussione le inevitabili mancanze che ogni
test contiene.

Per raccogliere i dati del trattamento che ricevono gli utenti facendone una copia, abbiamo costruito un'**estensione del
browser** che colleziona quello appare all'utente.

**Rilasciamo** i dati dei nostri esperimenti per permettere ad altri gruppi di ricerca di verificare o confutare le
nostre analisi, e perché possano usare lo strumento. L'algoritmo continua a cambiare, il nostro software è appositamente
rilasciato sotto licenza libera perchè sia utilizzabile, solo collettivamente possiamo interrogarci su quali sistemi
debbano regolare i nostri discorsi online, l'alternativa, sarebbe tecnocrazia.

Abbiamo scoperto che l'acquisizione dei dati è il momento più sensibile (delicato?).

Nella ricerca di nuove metriche per capire e misurare l'algoritmo, abbiamo iniziato ad usare l'espressione: **Dieta
informativa**, Per dare un nome al regime cui Facebook sottopone i suoi utenti. A prescindere dai valori utilizzati da
Facebook, il nostro scopo è evidenziare ed eventualmente trasformare l'arbitrarietà con la quale l'algoritmo decide per
nostro conto.

Abbiamo incontrato, durante questo esperimento, anche il concetto di Diversità informativa. Ossia la probabilità con
la quale l'algoritmo proponga contenti che l'utente non hai mai visto prima. Possiamo dedurre che l'esposizione a
contenuti diversi o la riproposizione di contenuti già visti impatti direttamente sulla diversità di opinioni al quale
l'utente viene esposto e di conseguenza nella percezione della pluralità. Abbiamo chiamato questa variabile di proposta
di contenuti sempre diversi **Diversità informativa**.

Per meglio analizzare la **filter bubble**, abbiamo osservato come i giornali online vengono trattati da Facebook,
constatando che l'algoritmo non considera solamente le preferenze, cioé i like. Come neppure le risorse investite dalle
attività commerciali, che avrebbero dato il ripetersi delle stesse proporzioni di proposte agli utenti. Sembra che il
giornale più avvantaggiato sia quello centrista, come se l'algoritmo penalizzasse le posizioni più radicali, e nel
verificare che le pagine del giornale più seguito sono proposte più spesso, si potrebbe dedurre che l'algoritmo preserva
lo status quo, ma purtroppo non abbiamo ancora una chiara determinazione; la metodologia serve a verificare ipotesi che
poi impattano sulle nostre diete informative individuali.

Va fatto notare che **Facebook attivamente ostacola analisi indipendenti**. E questo non avviene tramite pressioni legali,
come nel caso di Spotify. La resistenza di Facebook usa il suo strapotere tecnologico. Rendendo difficile la creazione
di utenti sperimentali, individuando attività che si discostano dal comune, o nascondendo nel loro codice HTML porzioni
atte solo ad offuscare i dati che ci servono per i raffronti. **La protezione dell'algoritmo non avviene con meccanismi
legali, ma tecnologici**.

Ad oggi non sappiamo ancora giudicare gli algoritmi ed il loro potere. Analisti come noi stanno inventando tassonomie e
le mettono a verifica, ma quando la società civile pone delle richieste alle piattaforme, queste sono sempre volte a
risolvere un problema di tipo politico, come il rimuovere la disinformazione o i messaggi d'odio, o evitare il bullismo
online. Pensiamo che da una parte non è accettabile che si deleghi una responsabilità censoria ad un'azienda privata, e
inoltre che non vada accettata questa tendenza a delegare lo spirito critico. Le dinamiche di potere delle reti vanno
affrontate decostruendo e scorporando questi poteri. **Dobbiamo poter usare i dati in modo costruttivo**.

# Proposta

Tracking exposed offre strumenti, esperienze, metodo e discorso sull'analisi degli algoritmi. Dall'analisi passiva
delle esperienze personalizzate, fino alla declinazione degli effetti sugli esperimenti effettuati.

Fondamentale la comprensione che qualunque risposta al funzionamento di un algoritmo non può avere una risposta
soddisfacente da parte della stessa entità che questo algoritmo ha inventato ed utilizza, per evidente conflitto
interessi. Non è pensabile che sia la stessa ditta che produce l'algoritmo ad autoregolamentarsi, inoltre l'algoritmo é
cosí personalizzato, geo-localizzato e regionalizzato per cui la versione che *gira* in un luogo, non vale per l'altro.

Le competenze e le esperienze pratiche di trex sono condivisibili: utilizzabili da ricercatori come metodo di analisi o
da un'entità produttrice di contenuti che voglia capire le logiche di valutazione e di un partner che voglia supportare
l'esperienza e il lavoro di Trex.

Offriamo test affidabili, in un panorama dove anche i test inaffidabili vengono comunque ripresi e utilizzati e i
racconti aneddotici rischiano di essere il riferimento più ricorrente.

# Storia di Trex

## 2016: FBTREX

Claudio Agosti scrive fbtrex, un'estensione per navigatore che permette agli utenti di registrare i propri dati,
prodotti durante la navigazione su Facebook. (to be expanded)

## 2016: ALEX

In unione con il consiglio di ricerca Europeo e l'Università di Amsterdam nasce il progetto ALEX: Algoritmo
Esposto. Mirato a smascherare le funzionalità degli algoritmi di personalizzazione sulle piattaforme social media. In
un'ottica di data-attivismo nella pratica, ossia di intervento sul sociale. Stefania Milan, recipiente del EU concept
grant, scrive: "Generalmente, le analisi degli algoritmi non coinvolgono gli utenti finali, mancando una cruciale
opportunità per creare consapevolezza e cambiamento comportamentale. ALEX è uno strumento per ricercatori, giuristi,
legislatori e giornalisti per ottenere metodi e dati di analisi algoritmiche affidabili, ma ancora più per dare la
possibilità agli utenti di monitorare, paragonare e riflettere in modo indipendente sulla propria dieta informativa".

## 2017: Argentina

Il primo report investigativo è stato realizzato in Argentina durante il G20

***(qui ci vorrebbe una breve descrizione di cosa è stato fatto e com'è andata) dicendo in cosa è
consistito, su che scala e soprattutto cosa se ne è ricavato, per costruire una 'genealogia' di trex.

## 2018: Facebook e le elezioni italiane

Tra Gennaio e Marzo 2018 abbiamo realizzato l'esperimento di analizzare l'algoritmo di Facebook, affidandoci a una rete
di partecipanti e creando un'estensione per browser chiamata **facebook.tracking.exposed**. In passato avevamo fatto degli
esperimenti su scala minore, grazie ai quali abbiamo scoperto che le variabili che competono nella creazione delle
timeline di Facebook sono molteplici e che il momento dell'acquisizione dei dati è il momento più delicato. Due persone
con le stesse amicizie che seguono le stesse pagine, ma si collegano in un momento diverso della giornata, saranno
sottoposti a post diversi, ad esempio se tra un collegamento e l'altro un influencer abbia postato qualcosa. Di
conseguenza sarebbe inutile comparare i loro post senza tenerne conto e considerare solo l'algoritmo come unico
responsabile del tipo di post che vengono proposti. Nei mesi precedenti le elezioni abbiamo raffinato la nostra
metodologia in modo da minimizzare queste variabili. Il **metodo** usato è stato il seguente: abbiamo creato **6 profili**..

* 6 profili,
* Senza amici,
* Che seguono le stesse 30 pagine,
* E accedono a Facebook in modo automatico 13 volte al giorno, alla stessa ora.
* Trascorrendo ognuno la stessa quantità di tempo su Facebook.

Le 30 pagine sono state suddivise, in parti eguali, in 5 gruppi dall'orientamento politico dichiarato: Centro sinistra,
Destra, Estrema destra, Sinistra e Movimento 5 stelle. La scelta delle pagine più significative è stata effettuata da
noi, su base qualitativa.

La nostra estensione facebook.tracking.exposed ha raccolto una media di 50 post ad ogni accesso. Abbiano registrato come
Facebook ha informato questi 6 profili durante la campagna elettorale. Non badando alla pubblicità, ma all'algoritmo che
cura i contenuti. Nei primi giorni, ogni profilo ha ricevuto più o meno gli stessi contenuti, ma in seguito abbiamo
fatto divergere i profili, facendogli esprimere dei like per indirizzare i profili verso ognuno dei diversi gruppi
politici. Ogni profilo si è così indirizzato verso una collocazione politica definita, tranne uno, che non ha mai
espresso approvazione a nessun contenuto. In osservanza alla teoria della filter bubble, ci siamo aspettati di
constatare una crescita di contenuti relativi all'affiliazione politica espressa dal profilo.

Abbiamo poi aggregato i post per tipo di media: testo, immagini e video. Ed effettuata una somma giornaliera dei
contenuti unici. Abbiamo evinto che **Facebook ha deciso che alcuni utenti avrebbero visto più foto che testi**; Queste
percentuali sono state costanti giorno per giorno. Sono bastati pochi like di differenza perché l'algoritmo decidesse di
relegare uno dei nostri profili ad una timeline dove le foto dominano sui testi e inoltre, alcuni contenuti sono stati
ripetuti e riproposti a seconda dell'orientamento ideologico.

Usando il **metodo di analisi per tipologia di contenuti** siamo stati in grado di intuire alcune variabili utili a
capire come funziona l'algoritmo di Facebook, tra queste:

* Quante volte un contenuto viene ripetuto a seconda dell'orientamento ideologico dell'utente.
* La dinamica di esposizione algoritmica alle diverse tipologie di contenuti: foto, video, testo, status update.
* La differenza fra tempo di pubblicazione e di visualizzazione di un contenuto.

In particolare, è dimostrato come l'utente orientato a estrema destra abbia subito una *discriminazione*, ossia non gli
sono stati proposti, molti contenuti testuali.  Ha invece ricevuto una considerevole quantità di contenuti fotografici,
ripetuti. Venendo così sottoposto a una dieta informativa considerevolmente diversa da quella degli altri
profili. Questa scoperta è stata considerata nell'ambito degli studi sulla bolla informativa che si interrogano se la
filter bubble condizioni più marcatamente coloro che si pongono alle posizioni estreme dello schieramento
politico. Inoltre, dopo il cambio di algoritmo annunciato da Facebook, gli schemi di ripetizione sono variati in modo
significativo, alzando la media di post ripetuti, ma stavolta su tutto l'arco del posizionamento politico.

Il nostro studio ci ha permesso non solo di osservare le differenze numeriche dei post pubblicati dalle fonti in ogni
gruppo ideologico, ma ha fornito una **prova storica su come l'algoritmo impatti sull'esposizione ai contenuti e di come
queste dinamiche siano in evoluzione**. I nostri dati non ci permettono di speculare su perché questo avvenga, ma
dimostrano che queste sproporzioni esistono e che sono inverificate e inspiegate.

Nei **futuri sviluppi** tenteremo di fare miglior uso dei profili, sia sotto l'aspetto tecnico del tracciamento
comportamentale che di analisi della composizione ideologica del quadro di riferimento mediatico. Ad esempio,
incrociando i dati raccolti in situazioni diverse e applicando ulteriori parametri, come il considerare i like non solo
delle pagine, ma dei post individuali.

l'Analisi condotta su Facebook contribuisce a dimostrare come l'algoritmo che fornisce contenuti personalizzati agisca
come Social Policy e cioè contribuisca a formare il consenso; di conseguenza il suo funzionamento non può restare
segreto e continuare a conciliarsi con una democrazia, ma deve essere trasparente, verificabile e sottoposto allo
scrutinio pubblico.

Contesto dell'analisi e metodologia sono descritte in dettaglio nella pubblicazione in inglese:  
[The Influence IndustryPersonal Data and Political Influence in Italy:](https://cdn.ttc.io/s/ourdataourselves.tacticaltech.org/ttc-influence-industry-italy.pdf)


## 2019: Youtube

Summer School dell'università di Amsterdam

Abbiamo deciso di non utilizzare i canali d'accesso per programmatori forniti da Youtube, che permettono di scaricare le
informazioni associate ai video, tra le quali una lista di 60 video correlati, avendo constatato che questa lista non
corrisponde e non contiene i video che vengono in effetti proposti. A questo proposito, ricordiamo che non è plausibile
chiedere alla stessa entità, in questo caso Alphabet, di fornire strumenti di analisi per quanto riguarda la sua stessa
attività, dato che il loro modello di business si basa sulla segretezza di questi strumenti.

Difatti nel test che abbiamo effettuato usando le YouTube API per scaricare la lista dei 60 video raccomandati dopo la
visione di un popolare video musicale, con 10 studenti che hanno utilizzato il loro browser per accedere allo stesso
video, abbiamo potuto verificare che solo una piccola parte dei 200 video, ossia i 20 per ogni utilizzatore che vengono
proposti nella colonna destra di Youtube, si sovrappone alla lista dei video raccomandati dalle API di Youtube.

Abbiamo ripetuto lo stesso test con dei browser puliti e con browser già utilizzati, ricavando il prevedibile ma
dimostrabile risultato che **solo quando si ha un browser veramente pulito si ha un trattamento meno personalizzato**.
Difatti la navigazione con browser pulito ha prodotto una notevole quantità di risultati in comune tra i 10 studenti,
mentre lo stesso test con browser usato ha prodotto video suggeriti in comune e per la maggior parte risultati e
personali, cuciti su misura della loro storia pregressa.

Il risultato di questa analisi quantitativa, solo volta a capire quanto di quello che vediamo è in comune alle persone
attorno a noi, non considera ancora il contenuto: politico, intrattenimento, informativo, eccetera: cosa che necessita
di ulteriori elementi contestuali.

## 2019: Pornhub

Mi ridate il link alla pagina su pad che avevate scritto su pornhub? [g]

L'esperimento si è tenuto in gennaio e marzo 2019 su base volontaria. È Stato chiesto ai partecipanti di installare la
nostra estensione per browser **pornhub.tracking.exposed** e di visitare pornhub alla pagina dei video proposti, poi
direttamente un video che è presente da più di dieci anni e poi un video recente. Infine, di ritornare alla lista dei
video proposti prima di tornare alla homepage.

Pornhub.tracking.exposed è un'estensione per navigatore web che permette di studiare e analizzare le implicazioni degli
algoritmi di personalizzazione attraverso la collezione e la comparazione dell'esperienza individualizzata che gli
utenti di Pornhub ricevono.

Gli algoritmi di personalizzazione hanno il potenziale di formare la percezione pubblica (il super-io) e pornhub
dichiara di implementare questo tipo di personalizzazione. Un algoritmo che attraverso l'analisi di dati indirizza
contenuti legati alle scelte sessuali, potrebbe influenzare il comportamento sessuale delle persone nella vita
reale. L'obiettivo primario è di scoprire quanta esperienza personalizzata varia tra persone che compiono la stessa
azione, tenendo conto anche della geo localizzazione e della relazione tra i video.

Gli esperimenti sono stati effettuati su base volontaria non localizzata, la prima in gennaio 2020, alla data prevista
per il secondo, marzo 2020, la pandemia ha spostato le priorità e ci stiamo lavorando. Abbiamo identificato alcune
variabili: **Pornhub utilizza i cookie e tracking code per indirizzare le persone attraverso i feticci**. La pagina
principale e i video raccomandati dipendono dalla navigazione precedente.

Abbiamo bisogno di nuovi esperimenti.

* voglio notare e mi piacerebbe ampliare come il distanziamento sociale, misura presa dalle nazioni democratiche per non
  lasciare indietro le persone più fragili, naturalmente genera isolamento e ipo-tele (aumento ipertrofico di attività
  telematica) in un contesto sociale impreparato. All'inizio di questa circostanza emergenziale l'utilizzo e dunque
  l'impatto delle piattaforme [non so se citare il termine GMAFIA] è aumentato e l'esperimento potrex, analizzando le
  conseguenze algoritmiche della fruizione del sesso telematico, acquista maggiore contemporaneità (come tutta
  trex). (to be rephrased)


## 2019: Amazon

**amazon.tracking.exposed** si è proposta di studiare l'algoritmo di Amazon, società nata come vendita di libri ed oggi
monopolista occidentale della vendita online.

Abbiamo scoperto che i prezzi cambiano a seconda di chi cerca un prodotto.

Contattati dalla trasmissione di RAI News 24 Petrolio, abbiamo sviluppato un software, un'estensione per navigatore
chiamata amazon.tracking.exposed, per analizzare il portale di vendita Amazon. La nostra ricerca si è sviluppata attorno
alla trasparenza nel trattamento dei dati personali, e **abbiamo registrato una pratica di vendita anomala**, seppur già
nota: la vendita del medesimo prodotto a prezzi differenti in base all'utente che effettua la ricerca. È possibile che
sia il venditore responsabile di questa differenza.

I due aspetti di Amazon che siamo stati in grado di collezionare ed analizzare sono: le risposte ad una ricerca e i
prodotti correlati che la piattaforma mostra quando si visualizza un prodotto. In questo caso, è stato chi ha
effettuato la ricerca che ha dovuto tenere traccia del test, considerando se avesse fatto login o meno, la presenza di
cookie e scaricando poi i dati in formato CSV. Abbiamo ricavato che la personalizzazione dipende sia da dati personali
(profilazione), sia da dati non personali, come il Codice di Avviamento Postale, che viene rilevato e considerato.  Per
realizzare questa ricerca abbiamo allora uniformato il più possibile queste variabili per avere una situazione di
partenza e personalizzazione controllata, durante la quale abbiamo fatto azioni di ricerca con diverse azioni e diversi
utenti, ad esempio: cercare, inserire e togliere prodotti dal carrello. Infine abbiamo eseguito la stessa ricerca e i
risultati per gli utenti sono stati differenti, ne abbiamo ricavato che le azioni sul sito web, chiamate da Amazon
Clickstream, vengono considerate dall'algoritmo di Amazon per personalizzare le risposte.

## 2020: le Elezioni presidenziali americane

Descrizione

# Attualità - cronaca - fatti

[] Questa è eventualmente ancora da fare. Imho comincerei ad elencare brevemente tutti i casi che si decidono di citare
in modo poi da capire come elencarli/proporli.

Casi di cronaca in cui gli algoritmi falliscono, di quando le compagnie hanno promesso invano che li avrebbero usati
come sistema di regolamentazione.  (l'attuale meccanismo di reportistica, analisi, racconto, è inadeguato)

* Tay, il Bot di Microsoft su twitter che diventò Nazi
* Il caso cambridge analytica
* il caso facebook su hate speech
* citazioni altri casi (a pioggia)
* Estrarrei gli eventi citati qui. <https://eu19.tracking.exposed/page/data-activism/>
*

Va aggiunta parte sull'attualità: (epidemia che aumenta la tele-feresi, indebolisce, isola, allontana. Insomma la
situazione emergenziale aumenta la necessità di decostruire i sistemi di automatizzazione).

* https://edition.cnn.com/2020/08/06/business/instagram-biden-trump-algorithm/index.html


# Documentario

io mi sa che farò un documentario[h][i][j] (a prescindere dai fondi), togliendovi la faticaccia di
autorappresentarvi. (please, let me highlight what i see).  Naturalmente finisce in musicòl.

* Trex è un'azione diretta contro l'egemonia algoritmica (pioneristica dato il momento storico - medioevo).
  Motivi, metodo, risultati e aspirazioni.
* Background: transparency, cypherpunk e artigiani del cuoio.
* Finale musicale.

Appunti vari che mi capisco da me: (algoritmo captivo, vari momenti, metametodo, gli automatismi -ambito legale come riconoscimento scrittura -

# Riferimenti e Corrispondenze

## In Musica (secondo intermezzo)

* [Schermo splendente - Ustmamò](https://www.youtube.com/watch?v=tZk-mEiAYoY)

Schermo splendente, svanire non puoi, bianco candore, buio grigiore, luce perenne non può sbiadire. La tua saggezza non
prevede limite.  
Schermo delle mie brame, messaggero perfetto, luce di verità.  
Schermo splendente, re mida tu sei, dietro il tuo vetro, dentro la testa spegni ed accendi quello che resta, pura
memoria che non vede limite.  
Schermo delle mie brame, condottiero virtuale, luce di verità.  
Prendi, trasforma, dissolvi, scomponi, inventa di nuovo, progetta, disponi.  
Luce dei miei occhi, specchio degli specchi. Mente che riflette, ripetutamente.  

* [Got my mind set on you - James Ray sings George Harrison](https://www.youtube.com/watch?v=A1E6xvM7PeA)

it's gonna take money, a whole lotta spending money.  
it's gonna take plenty of money, to do it right.  
And it's gonna take time, a whole lot of precious time.  
It's gonna take patience and time, to do it right, child.  

## In Video

* [Hacking democracy, Michaels and Ardizzone, 2006](https://en.wikipedia.org/wiki/Hacking_Democracy)

* [Channel4, reportage on Cambridge Analytica, 2018](https://www.channel4.com/news/cambridge-analytica-revealed-trumps-election-consultants-filmed-saying-they-use-bribes-and-sex-workers-to-entrap-politicians-investigation)

* [The Trouble with Harry (la Congiura degli innocenti), Alfred Hitchcock, 1955](https://en.wikipedia.org/wiki/The_Trouble_with_Harry)

# Link

* Complessità dell'analisi

<https://docs.google.com/document/d/1NRF0FfH0SoNPQadlYZs6tLfpWNpJu63_I8S04QnVH7U/edit>

<https://policyreview.info/articles/news/personalisation-algorithms-and-elections-breaking-free-filter-bubble/1385>

* Vedere le cose dal punto di vista di chi osserva/subisce

<https://www.cjr.org/the_media_today/youtube-radicalization.php>

<https://www.cnbc.com/2019/12/30/critics-slam-youtube-study-showing-no-ties-to-radicalization.html>

* Double standard

<https://twitter.com/mark_ledwich/status/1210743158184693760>

* Effetti e Problema di trasparenza

<https://ffwd.medium.com/youtubes-deradicalization-argument-is-really-a-fight-about-transparency-fe27af2f3963>

* Il rilascio dati è sempre incompleto

<https://eu19.tracking.exposed/page/data-activism/>

* Anche i test inaffidabili vengono ripresi

<https://twitter.com/mark_ledwich/status/1224825958131105792>

* Reproducibility checklist

<https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf>

* Le tre metodologie di trex comparate

<https://youtube.tracking.exposed/data/>

* Facebook

<https://cdn.ttc.io/s/ourdataourselves.tacticaltech.org/ttc-influence-industry-italy.pdf>

* Amazon

<https://docs.google.com/document/d/1m8L4UAxgNZNeEMUGuHOy7e44wLYz7IUrAOevBc-5Y3k/edit>

<https://docs.google.com/document/d/1VxVBTC8x4YrQjLnKrqFdEA323Oq43mnInSNM5ya2jV4/edit>

<https://docs.google.com/presentation/d/1eLqvFDGJh75OeDI9eYxEl9T9-uiZ-0xiWGshzlapojs/edit#slide=id.p>

* alex

<https://algorithms.exposed/>

August 2020 report (Note  internal report = seldom proofread.)

<https://tracking.exposed/monthly/august-2020/>

* Data activism

<https://eu19.tracking.exposed/page/data-activism/>

# Note di scrittura

* Unificare lo stile: "noi facciamo" o "trex fa" (al momento è misto)
Quale usare?

* Mettere in relazione Metodo e Casi e Scoperte (via link, grafo, mindset o grafica)

* La parte dei progetti futuri:
Fragorosamente assordante :) la mancanza di un progetto inerente al covid-19, non foss'altro che la comparazione tra dati pubblici e pubblicati.

* Non so se ha una sua utilità, la considerazione che nel dibattito tra se Youtube radicalizzi o de-radicalizzi, quali
  tipo di utenti ecc potrebbe essere trattato come un falso dibattito, in quanto in tutti i casi influenza
  politicamente.

* Questa pagina needs help
  Quick tour on yt - TREX technology the reference manual for researchers and analysts
  https://youtube.tracking.exposed/data/

